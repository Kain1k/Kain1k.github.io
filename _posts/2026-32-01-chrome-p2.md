---
title: "Chrome Exploitation Part 2: Security Architecture and Compiler Pipeline"
date: 2026-01-31 02:00:00 +0800
categories: [Browser Exploitation, Chrome Exploitation]
tags: [Chrome Exploitation, Browser Exploitation]
---
In Part 1, we covered the basic concepts of the V8 engine. Building on that, Part 2 will discuss Chrome’s security architecture and its attack surface, with a primary focus on the compiler pipeline.

## V8’s Isolate and Context

### Isolate
An isolate is a concept of an instance  or “virtual machine” in V8 which represents one JavaScript execution environment; including a heap manager, a garbage collector, etc. In Blink, isolates and threads are in 1:1 relationship. One isolate is associated with the main thread. One isolate is associated with one worker thread. An exception is a compositor worker where one isolate is shared by multiple compositor workers.

### Context
A context is a concept of a global variable scope in V8. Roughly speaking, one window object corresponds to one context, and since each frame has a window object, there are potentially multiple contexts in a renderer process. For example, `<iframe>` has a window object different from a window object of its parent frame. So the context of the `<iframe>` is different from the context of the parent frame. Since these contexts create their own global variable scopes, global variables and prototype chains of the `<iframe>` are isolated from the ones of the parent frame. They are not directly shared between "contexts", even though they run in the same "process".

Here is an example:

```html
// main.html
<html><body>
<iframe src="iframe.html"></iframe>
<script>
var foo = 1234;
String.prototype.substr =
    function (position, length) { // Hijacks String.prototype.substr
        console.log(length);
        return "hijacked";
    };
</script>
</body></html>

// iframe.html
<script>
console.log(foo);  // undefined
var bar = "aaaa".substr(0, 2);  // Nothing is logged.
console.log(bar);  // "aa"
</script>
```
In relation to the isolate, the isolate and contexts have a 1:N relationship over the lifetime of the isolate - where that specific isolate or instance will interpret and compile multiple contexts.

This means that each time JavaScript need to be executed, we need to validate that we are in the correct context via `GetCurrentContext()` or we’ll end up either leaking JavaScript objects or overwriting them, which potentially can cause a security issue.

In Chrome, the runtime object `v8::Isolate` is implemented in [v8/include/v8-isolate.h](https://source.chromium.org/chromium/chromium/src/+/main:v8/include/v8-isolate.h;l=203) and the `v8::Context` object is implement in [v8/include/v8-context.h](https://source.chromium.org/chromium/chromium/src/+/main:v8/include/v8-context.h;l=45). Using what we know, from a high-level, we can visualize the runtime and context inherence in Chrome to look like so:

![](V8Isolate.png)

```
Renderer Process
└─ V8 Isolate
   ├─ Shared Heap
   ├─ GC (Marking, Sweeping, Compaction)
   ├─ JIT
   │  ├─ Ignition (bytecode interpreter)
   │  └─ TurboFan (optimizing compiler)
   ├─ Builtins (shared machine code)
   ├─ .... 
   ├─ Context A (main window)
   │  ├─ Global Object
   │  ├─ Global Scope
   │  ├─ Prototype Chains (per-context)
   │  ├─ Builtin Objects (per-context instances)
   │  ├─ ....
   │
   ├─ Context B (iframe #1)
   │  └─ (same internal structure as Context A)
   │
   └─ Context C (iframe #2)
      └─ (same internal structure as Context A)
```

## The Chromium Security Architecture   
Chromium uses a multi-process architecture to separate different tasks into different processes. This separation increases the browser’s stability and security because issues in one process (like a tab crashing) do not affect the others.

The Chromium Project image below provides a color-by-risk overview of the browser’s components.
![](chromium1.png)
In the diagram, we see several distinct process types:

 - **Browser Process**: Manages the UI, network and disk I/O, and coordinates all other processes. Runs with full user privileges and owns persistent user data.

 - **Renderer Processes**: Render web pages and execute JavaScript. Each site typically runs in its own isolated renderer with strict sandboxing.

 - **GPU Process**: Handles graphics and GPU-related tasks separately to improve performance and security. Runs with minimal privileges.

 - **Utility Process**: Performs specific auxiliary tasks (e.g., printing) and may run sandboxed or unsandboxed depending on its role.

 - Lastly, the PPAPI (Pepper Plugin API) Broker Process and the NaCl (Native Client) Loader Process are two components in Chromium’s architecture that manage different aspects of running code within the browser.

For each process, Chromium enforces two security measures: sandboxing and the principle of least privilege.

- Each renderer and plugin process is run within a sandbox, a restricted environment that limits the process’s ability to read and write to the disk, interact with the OS, or communicate over the network. This containment strategy significantly reduces the risk of malicious code escaping the browser and affecting the entire user’s system.

- In addition, each process operates with the least privilege necessary to perform its functions. For example, renderer processes have very limited access to system resources, and any action that requires more privileges (like saving a file) must go through the browser process, which acts as a security gatekeeper.

Within the scope of this article, we’re going to take a closer look at the renderer process in Chromium. The renderer process is critical to the browser’s security as it’s responsible for drawing web pages and executing untrusted JavaScript code, making it a prime target for exploits.

As we know, V8 is the JavaScript and WebAssembly engine powering renderer process in Chromium. Understanding V8 pineline is essential because any issues here directly impact the security and stability of the renderer process.

But ... Before diving into the V8 compiler pipeline, I want to outline the overall attack surface of Chrome based on what I currently know, as a basis for deeper research in the future. Browsers are complex software with large codebases, so vulnerabilities are not confined to a few JIT bugs and common renderer process exploits.
## Attack Surface
The first attack surface we familiar is `Renderer Process`. This process handles untrusted web content and is the primary target for initial compromise. Some of the vulnerabilities that can be found in:
 - JavaScript Engine (V8):
    - JIT Compilation Pipeline: Vulnerabilities often reside in the optimization pipeline, leading to type confusion where the engine treats data as the wrong type or out-of-bound.
 - Rendering Engine (Blink) & DOM:
    - Chrome using **Blink** as a rendering engine. DOM vulnerabilities are often related to the object lifetime, that can lead to Use-After-Free (UAF). 
    - Web Platform APIs (Extended Features), which is a part of Blink, may contain many potential vulnerabilities. For example, there have been several UAF and race condition vulnerabilities in WebAudio, WebRTC, WebGL and WebGPU.
 - And many, many more components you can’t really enumerate, such as garbage collection, WebAssembly, parsers, ...

But that’s not the end. To build a complete exploit chain, we still need to escape the sandbox. Typical targets include the `Browser Process` and the `GPU Process`. Vulnerabilities can be exist in IPC (inter-process communication) and may this depends on the platform the browser is running on, such as Android or desktop systems.

At this point, it’s clear that Chrome has far more attack surfaces than can be covered in an introductory series like this. Now, let's return to the main topic of the blog as I mentioned earlier.
## The V8 Compiler Pipeline
I always enjoy historical stories, so let’s travel back in time to 2008. V8 was designed with simplicity in mind and involved only three major steps.
![](pineline.jpg)
It all begins with JavaScript source code that is fed into the compiler pipeline. This JS code is first processed by a parser, which checks syntax and structures it into an Abstract Syntax Tree (AST). The AST represents the program’s syntax in a hierarchical tree format, with each node representing different constructs in the source code.

Following this, the AST is transformed into executable code through a process called code generation (codegen), initially producing semi-optimized machine code. This code is already executable but not fully optimized, allowing for quick, but not fully efficient, execution.

As websites became more complex, so did their JavaScript. It then became necessary to improve browser performance.

The idea of Just-in-time (JIT) compilation helped meet those demands for performance. This approach differs from traditional compilation, which converts code to machine language before it is executed on a target device (known as Ahead-Of-Time, or AOT, compilation). Conversely, JIT compilation is a technique where computer code is compiled into machine language at runtime rather than prior to execution.

JIT compilers aim to combine the speed of compiled code with the flexibility of interpretation, often used in environments where code may change at runtime or where fast startup time is needed, as in modern browsers.

The concept of JIT compilation became popular in the 90s with the rise of virtual machine environments like the Java Virtual Machine (JVM) and the .NET Common Language Runtime (CLR). These environments use JIT compilation to execute platform-independent code like Java bytecode or CIL code in the .NET framework.

In 2010, the first JIT compiler named Crankshaft was launched and later, in 2014, replaced by the well-known and more performant TurboFan. Looking at the images below, we can see the differences between the compiler pipelines from 2010 to 2016.

![](pineline2.jpg)
![](pineline3.jpg)
![](pineline4.jpg)

I mean ...

![](tl.jpg)

TurboFan utilizes a flexible intermediate representation allowing for sophisticated code optimizations. It excels in optimizing new language constructs such as classes, destructuring, and default parameters, which its predecessor Crankshaft struggled with. Maintaining an optimization pipeline like the one in 2016 is simply too complex.

For the past decade, TurboFan has been a part of the V8 pipeline, along with its counterpart interpreter, Ignition. Together, they define the well-established modern JIT pipeline, as depicted below.

![](pineline5.jpg)

Ok, how it works.

As shown in the image above, Ignition (the fastest interpreter among all raw bytecode execution speed tools) generates bytecode and executes it. When the code is executed frequently enough and becomes "hot", TurboFan is activated and eventually produces highly optimized machine code.

![](pipeline-detail-v8.jpg)

Here we have a problem: there is a trade-off between quickly getting code to run, or taking some more time but eventually running the code with optimal performance. An interpreter can produce bytecode quickly, but bytecode is generally not very efficient. An optimizing compiler on the other hand takes a little longer, but eventually produces much more efficient machine code.

![](tradeoff-startup-speed-v8.jpg)

This trade-off between startup latency and execution speed is the reason why some JavaScript engines choose to add optimization tiers in between. For example, SpiderMonkey adds a Baseline tier in between the interpreter and their full IonMonkey optimizing compiler:

![](tradeoff-startup-speed-spidermonkey.jpg)

Back to V8, we can see that there is a large gap between Ignition and TurboFan. Code that falls into this gap is considered not "hot" enough, or we can call it ‘warm’. It runs frequently enough that interpreting bytecode becomes a bottleneck, but not frequently enough to trigger TurboFan.

At the time this blog was written, V8 had introduced additional optimization tiers to fill this large gap, namely SparkPlug and Maglev. Together, they handle ‘warm’ code much more effectively. As a result, we get a more complete pipeline that allows Chrome to achieve high performance from start to finish. The pipeline when code becomes “hot” can be visualized in the image below. The diagram shows the execution tier-up process: code starts in Ignition, moves to Sparkplug, and is eventually promoted to Maglev and TurboFan as it runs more frequently.

![](v8_full_pineline.png)

While the flow is sequential, technically Maglev and TurboFan generate optimized code directly from the Ignition Bytecode (using feedback from Sparkplug), not from the previous tier's machine code. Maglev and TurboFan use speculative optimizations to generate faster machine code based on data collected at runtime, which is why they must support deoptimization back to Ignition.

![](pipeline_sum.png)

One more thing you should know: There is a main reason why JavaScript engines don’t just "optimize" everything. Generating optimized machine code takes a long time, and it also consumes more memory.

![](tradeoff-memory.jpg)

In summary, modern JavaScript engines should use multiple optimization tiers because of a fundamental trade-off between quickly generating code, as with an interpreter, and generating fast code with an optimizing compiler. This is a scale, and adding more optimization tiers allows for finer-grained decisions at the cost of additional complexity and overhead. There is also a trade-off between optimization level and memory usage, which is why JavaScript engines only optimize hot codes/functions.

And now, let’s go into the details of each component in the pipeline to better understand them.

### The Parser
We mentioned that the first step involves the parser. But what’s actually does a parser? The parser’s job is to translate your raw code into something the computer can understand. It happens in three main steps:
1. **Lexical Analysis (Tokenization)**: The parsing process starts with lexical analysis. Here, the V8 engine scans your raw code character by character and groups them into "tokens" based on JavaScript syntax rules. These tokens are the smallest meaningful units of the program, such as keywords, identifiers, and operators. Essentially, the engine is breaking your code down into individual vocabulary words.

2. **Syntax Analysis**: Once tokenized, the engine moves to syntax analysis. It checks the arrangement of these tokens against JavaScript’s grammar rules ([ECMAScript](https://tc39.es/ecma262/)) to ensure the code is valid. This process builds a Parse Tree that maps out the structure of your code. If you have any syntax errors, like a missing bracket or a typo in a keyword, this is the exact stage where the engine detects and reports them.

3. **Parse Tree to Abstract Syntax Tree**: (AST) While the initial Parse Tree is accurate, it is often too detailed and "verbose" for the computer to process efficiently. To fix this, the engine transforms it into an Abstract Syntax Tree (AST). The AST removes redundant information, creating a compact and clean representation of your code’s logic that is ready for execution.

We can inspect the AST using V8’s debugging tool, aka d8. The d8 shell is mainly used for debugging and testing V8 features, and it supports various command-line flags that allow us to inspect V8’s internal processing of JavaScript code, such as printing the AST or tracing function optimizations. d8 plays an important role in testing and validating code while building a PoC for exploitation.

I will write another post about how to build d8 on Windows, especially for old CVEs.

I have a JavaScript code example below.

```js
function square(obj) {
    return obj.x * obj.x;
}
square({x:42});
```

We can save its content as "square.js" and invoke the d8 instance with the `–-print-ast` flag in order to print out the AST and load the saved JS file from within d8.
```
C:\BE\CVE-2018-17463\v8\v8\out\x64.debug>d8 --print-ast
V8 version 7.1.0 (candidate)
d8> load('square.js')
...
[generating bytecode for function: square]
--- AST ---
FUNC at 15
. KIND 0
. SUSPEND COUNT 0
. NAME "square"
. PARAMS
. . VAR (000001F0E807FE80) (mode = VAR) "obj"
. RETURN at 27
. . MUL at 40
. . . PROPERTY at 38
. . . . VAR PROXY parameter[0] (000001F0E807FE80) (mode = VAR) "obj"
. . . . NAME x
. . . PROPERTY at 46
. . . . VAR PROXY parameter[0] (000001F0E807FE80) (mode = VAR) "obj"
. . . . NAME x
```
I won't go into detail. All the AST expression statements can be found under the [header](https://source.chromium.org/chromium/chromium/src/+/main:v8/src/ast/ast.h) definition in the V8 codebase.

### Ignition

The Ignition interpreter itself consists of a set of bytecode handler code snippets, each of which handles a specific bytecode and dispatches to the handler for the next bytecode. These **bytecode handlers** are written in a high level, machine architecture agnostic form of assembly code, as implemented by the `CodeStubAssembler` class and compiled by TurboFan’s backend when the browser is compiled.

An example of the LdaZero or “Load Zero to Accumulator” bytecode handler from `v8/src/interpreter/interpreter-generator.cc` can be seen below.

```cpp
IGNITION_HANDLER(LdaZero, InterpreterAssembler) 
{
  TNode<Number> zero_value = NumberConstant(0.0);
  SetAccumulator(zero_value);
  Dispatch();
}
```

As such, the interpreter can be written once and uses TurboFan to generate machine instructions for each of the architectures supported by V8. When the interpreter is enabled, each V8 isolate contains a global interpreter dispatch table, holding a code object pointer to each bytecode handler, indexed by the bytecode value. Generally, this dispatch table is simply just an **enum**. These bytecode handlers can be included in the startup snapshot and deserialized when a new isolate is created.

In order to be run by the interpreter, a function is translated to bytecode by a `BytecodeGenerator` during its initial unoptimized compile step. The `BytecodeGenerator` is an `AstVisitor` which walks the function’s AST emitting appropriate bytecodes for each AST node by calling the `GenerateBytecode` function. This bytecode is associated with the function as a field on the `SharedFunctionInfo` (SFI) object, and the function’s code entry address is be set to the `InterpreterEntryTrampoline` builtin stub.

When the function is called at runtime, the `InterpreterEntryTrampoline` stub is entered. This stub set up an appropriate stack frame, and then dispatch to the interpreter’s bytecode handler for the function’s first bytecode in order to start execution of the function in the interpreter. The end of each bytecode handler directly dispatches to the next handler via an index into the global interpreter table, based on the bytecode. 

#### Interpreter Code Execution
As mentioned above, when a function is executed, `InterpreterEntryTrampoline` is invoked and sets up an appropriate stack frame. So how does this stack frame get generated?

The Ignition interpreter is a register-based interpreter. These registers are not traditional machine registers, but are instead specific slots in a register file which is allocated as part of a function’s stack frame - in essence they are “virtual” registers. Bytecodes can specify the input and output registers on which they operate through bytecode arguments, which immediately follow the bytecode itself in the `BytecodeArray` stream. 

Bytecode will operate on these registers by specifying it in its operands, and Ignition will then load or store data from the specific stack slot that is associated with the register. Since register indexes map directly to the function stack frame slots, Ignition can directly access other slots on the stack, such as the context and the arguments that were passed in with the function.

The interpreter stack frame is built by the `InterpreterEntryTrampoline` builtin stub, which pushes the fixed frame values (Caller PC, Frame Pointer, JSFunction, Context, Bytecode Array and Bytecode Offset) and then allocates space in the stack frame for the register file (the BytecodeArray object contains an entry which tells the builtin how large a stack frame the function requires). It then writes undefined to all the registers in this register file, which ensures the GC doesn’t see any invalid (i.e., non-tagged) pointers when it walks the stack. This then starts the execution or “interpretation” of the function by Ignition which is handled within the [v8/src/builtins/x64/builtins-x64.cc](https://source.chromium.org/chromium/chromium/src/+/main:v8/src/builtins/x64/builtins-x64.cc) source file.

![](stackframe.jpg)

As you can see, we have the functions arguments in red, and it’s local variables and temporary variables for expression evaluation in green.

First, the thing we care about is the [JSFunction](https://source.chromium.org/chromium/chromium/src/+/main:v8/src/objects/js-function.h) (also known as a closure). A JSFunction contains many things, but we mainly focus on a few of them, such as the context, the `SharedFunctionInfo` object, and the [Feedback Vector](https://source.chromium.org/chromium/chromium/src/+/main:v8/src/objects/feedback-vector.h). The `Feedback Vector` also contains several interesting fields. An example of how this JSFunction looks like in memory can be seen below.

![](ClosureExample.png)

You might also notice that there is no accumulator register in the stack frame. Actually, Ignition still has an accumulator register, which is used by many bytecodes as implicit input and output register. This register is not part of the register file on the stack, but is instead maintained in a machine register (`rax` in x64 architecture) by Ignition. This minimize loading and storing repeatedly to memory for register operations. It also reduces the size of bytecode by avoiding the need to specify an input and output register for many operations. E.g., binary op bytecodes only require a single operand to specify one of the inputs, with the other input and the output registers being implicitly the accumulator register, rather than having to explicitly specify all three registers. This state register is pointed to by the [Frame Pointer](https://source.chromium.org/chromium/chromium/src/+/main:v8/src/execution/frames.h) (FP), which also holds the stack pointer and frame counter.

![](IgnitionFP.png)

The next thing we need to focus on is `Bytecode Array` pointer. This `BytecodeArray` represents a sequence of interpreter bytecodes for that specific function within the stack frame. Initially each bytecode is an enum where the index of the bytecode stores the corresponding handler. 

Well let’s take a quick look at the bytecode generated for `var num = 36`.
```
C:\BE\CVE-2018-17463\v8\v8\out\x64.debug>d8 --print-bytecode
V8 version 7.1.0 (candidate)
d8> var num = 42;
[generated bytecode for function: ]
Parameter count 1
Frame size 32
         0000024CF23A2162 @    0 : 12 00             LdaConstant [0]
         0000024CF23A2164 @    2 : 26 fa             Star r1
         0000024CF23A2166 @    4 : 0b                LdaZero
         0000024CF23A2167 @    5 : 26 f9             Star r2
         0000024CF23A2169 @    7 : 27 fe f8          Mov <closure>, r3
         0000024CF23A216C @   10 : 5f 28 01 fa 03    CallRuntime [DeclareGlobals], r1-r3
    0 E> 0000024CF23A2171 @   15 : a3                StackCheck
   10 S> 0000024CF23A2172 @   16 : 0c 2a             LdaSmi [42]
   10 E> 0000024CF23A2174 @   18 : 15 01 02          StaGlobal [1], [2]
         0000024CF23A2177 @   21 : 0d                LdaUndefined
   13 S> 0000024CF23A2178 @   22 : a7                Return
Constant pool (size = 2)
```
Take a look at the 1st line in the bytecode array, it stores `LdaConstant`. To the left of it we see `12 00`. The hex number `0x12` is the bytecode enumerator, which represents where the handler for that bytecode will be. This value may vary depending on the version.

Once that’s received, the `SetBytecodeHandler()` will be called with the bytecode, operands, and it’s handlers enum. This function is within the [v8/src/interpreter/interpreter.cc](https://source.chromium.org/chromium/chromium/src/+/main:v8/src/interpreter/interpreter.cc;l=120;bpv=0;bpt=1?q=SetBytecodeHandler&ss=chromium%2Fchromium%2Fsrc) file, example of that function is shown below.

```cpp
void Interpreter::SetBytecodeHandler(Bytecode bytecode,
                                     OperandScale operand_scale,
                                     CodeT handler) {
  DCHECK(handler.is_off_heap_trampoline());
  DCHECK(handler.kind() == CodeKind::BYTECODE_HANDLER);
  size_t index = GetDispatchTableIndex(bytecode, operand_scale);
  dispatch_table_[index] = handler.InstructionStart();
}

size_t Interpreter::GetDispatchTableIndex(Bytecode bytecode,
                                          OperandScale operand_scale) {
  static const size_t kEntriesPerOperandScale = 1u << kBitsPerByte;
  size_t index = static_cast<size_t>(bytecode);
  return index + BytecodeOperands::OperandScaleAsIndex(operand_scale) *
                     kEntriesPerOperandScale;
}
```

As you can see, `dispatch_table_[index]` will calculate the index of the bytecode from the dispatch table which is stored in a physical register, and eventually this will initiate or finalize the `Dispatch()` function to execute the bytecode.

The bytecode array also contains something called a “Constant Pool Pointer” which stores heap objects that are referenced as constants in generated bytecode, such as strings and integers. The constant pool is a `FixedArray` of pointers to heap objects. An example of this `BytecodeArray` pointer and its constant pool of heap objects can be seen below.

![](bcarray.jpg)

One more thing I want to mention before we continue, is that the `InterpreterEntryTrampoline` stub has some fixed machine registers that are used by Ignition. These registers are located within the [v8/src/codegen/x64/register-x64.h](https://source.chromium.org/chromium/chromium/src/+/main:v8/src/codegen/x64/register-x64.h;l=254) file.

A sample of these registers can be seen below, and comments are added to the ones of interest.

```cpp
// Define {RegisterName} methods for the register types.
DEFINE_REGISTER_NAMES(Register, GENERAL_REGISTERS)
DEFINE_REGISTER_NAMES(XMMRegister, DOUBLE_REGISTERS)
DEFINE_REGISTER_NAMES(YMMRegister, YMM_REGISTERS)

// Give alias names to registers for calling conventions.
constexpr Register kReturnRegister0 = rax;
constexpr Register kReturnRegister1 = rdx;
constexpr Register kReturnRegister2 = r8;
constexpr Register kJSFunctionRegister = rdi;
// Points to the current context object
constexpr Register kContextRegister = rsi;
constexpr Register kAllocateSizeRegister = rdx;
// Stores the implicit accumulator interpreter register
constexpr Register kInterpreterAccumulatorRegister = rax;
// The current offset of execution in the BytecodeArray
constexpr Register kInterpreterBytecodeOffsetRegister = r9;
// Points the the start of the BytecodeArray object which is being interpreted
constexpr Register kInterpreterBytecodeArrayRegister = r12;
// Points to the interpreter’s dispatch table, used to dispatch to the next bytecode handler
constexpr Register kInterpreterDispatchTableRegister = r15;
```

#### V8’s Bytecode
There are several hundred bytecodes within V8, and they are all defined within the [v8/src/interpreter/bytecodes.h](https://source.chromium.org/chromium/chromium/src/+/main:v8/src/interpreter/bytecodes.h) header file. As we know, each of these bytecodes specifies it’s input and output operands as registers to the register file. Additionally, many of the opcodes start with `Lda` or `Sta`, in the name, where the a stands for accumulator.

For example, let’s follow the bytecode definition for `LdaSmi`:
```cpp
V(LdaSmi, ImplicitRegisterUse::kWriteAccumulator, OperandType::kImm)
```
As you can see the `LdaSmi` will “Load” a value into the accumulator register. In this case it will load a `kImm` operand which is a signed byte, which coincides with the `Smi` or Small Integer in they bytecode name. In summary, this bytecode will load a small integer into the accumulator register.

Do note, that a list of operands and their types are defined within the [v8/src/interpreter/bytecode-operands.h header](https://source.chromium.org/chromium/chromium/src/+/main:v8/src/interpreter/bytecode-operands.h) file.

Let’s create a simple JavaScript function called incX which will increment an object’s property of x by one, and return it to us. The function should look like so.

```
C:\BE\CVE-2018-17463\v8\v8\out\x64.debug>d8 --print-bytecode
V8 version 7.1.0 (candidate)
d8> function incX(obj) { return 1 + obj.x; }
[generated bytecode for function: ]
Parameter count 1
Frame size 24
...
```
Let’s call that function with an object that has a value assigned to property x, and view the bytecode generated.

```
incX({x:13});
...
[generated bytecode for function: incX]
Parameter count 2
Frame size 8
   13 E> 000000F4704A527A @    0 : a3                StackCheck
   21 S> 000000F4704A527B @    1 : 0c 01             LdaSmi [1]
         000000F4704A527D @    3 : 26 fb             Star r0
   36 E> 000000F4704A527F @    5 : 28 02 00 01       LdaNamedProperty a0, [0], [1]
   30 E> 000000F4704A5283 @    9 : 32 fb 00          Add r0, [0]
   38 S> 000000F4704A5286 @   12 : a7                Return
Constant pool (size = 1)
000000F4704A5209: [FixedArray] in OldSpace
 - map: 0x01e6b6f82881 <Map>
 - length: 1
           0: 0x008798986941 <String[1]: x>
Handler Table (size = 0)
14
```

To start we see that `LdaSmi` is called to load a small integer into the accumulator register, which will be a value of 1. Next, we call `Star0` (`Star r0` in old version) which will store the value in the accumulator (as per the a) in register `r0`. So in this case we move 1 to `r0`.

The `GetNameProperty` (`LdaNamedProperty` in old version) bytecode gets a named property from `a0` and stores it in the accumulator, which will be the value of 13. The `a0` refers to the *i-th* argument of the function. So if we passed in, `a`,`b`,`x`, and we wanted to load `x`, the bytecode operand would state `a2` as we are the *2nd* argument within the function. In this case `a0` will look up the named property in the table to where the index 0 maps to `x`.

```
- length: 1
           0: 0x008798986941 <String[1]: x>
```

In short, this is the bytecode that loads `obj.x`. The other [0] operand is known as a feedback vector which contains runtime information and object shape data that is used for optimization by TurboFan.

Next, we `Add` the value in register `r0` to the accumulator, resulting in the value of 14. Finally, we call `Return` which returns the value of the accumulator, and we exit the function.

![](Ignition-BytecodeExample.gif)

### Sparkplug
In 2021, the V8 team introduced [Sparkplug](https://v8.dev/blog/sparkplug), a baseline Just-In-Time (JIT) compiler designed to improve performance over Ignition without the overhead of extensive optimizations that Turbofan requires.

Sparkplug is V8’s very-fast non-optimizing compiler which sits in between Ignition and TurboFan. Sparkplug is designed to compile fast. Very fast. So fast, that we can pretty much compile whenever we want, allowing us to tier up to Sparkplug code much more aggressively than we can to TurboFan code.

![](spark.jpg)

So, what makes Sparkplug so fast? First of all, it cheats; the functions it compiles have already been compiled to bytecode, and the bytecode compiler Ignition has already done most of the hard work like variable resolution, figuring out if parentheses are actually arrow functions, desugaring destructuring statements, and so on. Sparkplug compiles from bytecode rather than from JavaScript source, and so doesn’t have to worry about any of that. 

The second trick is that Sparkplug doesn’t generate any intermediate representation (IR) like most compilers do. Instead, Sparkplug compiles directly to machine code in a single linear pass over the bytecode, emitting code that matches the execution of that bytecode. This in general is known as 1:1 mapping. In fact, the entire compiler is a [switch statement](https://source.chromium.org/chromium/chromium/src/+/main:v8/src/baseline/baseline-compiler.cc;l=465;drc=55cbb2ce3be503d9096688b72d5af0e40a9e598b) inside a [for loop](https://source.chromium.org/chromium/chromium/src/+/main:v8/src/baseline/baseline-compiler.cc;l=290;drc=9013bf7765d7febaa58224542782307fa952ac14), dispatching to fixed per-bytecode machine code generation functions. We can see this implementation within the [v8/src/baseline/baseline-compiler.cc](https://source.chromium.org/chromium/chromium/src/+/main:v8/src/baseline/baseline-compiler.cc) source file.

An example of Sparkplug’s machine code generation function can be seen below.

```cpp
switch (iterator().current_bytecode()) {
#define BYTECODE_CASE(name, ...)       \
  case interpreter::Bytecode::k##name: \
    Visit##name();                     \
    break;
    BYTECODE_LIST(BYTECODE_CASE)
#undef BYTECODE_CASE
  }
```
So how does Sparkplug generate this machine code? Well, it does so by cheating again, of course. Sparkplug generates very little code of its own, instead Sparkplug just calls the bytecode builtins that are usually entered by the `InterpreterEntryTrampoline` and then handled within [v8/src/builtins/x64/builtins-x64.cc](https://source.chromium.org/chromium/chromium/src/+/main:v8/src/baseline/baseline-compiler.cc).

If you remember back to our `JSFunction` object during our talk about Ignition, you’ll remember that the closure linked to “optimized code”. In essence, Sparkplug will store the bytecode’s builtin there, and when the function gets executed, instead of dispatching to the bytecode, we call the builtin directly.

You can imagine SparkPlug like this. This is only an illustration and not entirely accurate, since it is difficult to observe or verify the machine code generated by SparkPlug in practice. Sparkplug is not a compiler in the traditional sense; it is a mechanical lowering of bytecode dispatch into machine code.

```js
// js
function add(a, b) {
  return a + b;
}

add(1, 2);

// bytecodes
00: Ldar a0
01: Ldar a1
02: Add
03: Return

// Sparkplug
mov rdi, [rbp + offset_for_a]      ; load a
mov rsi, [rbp + offset_for_b]      ; load b
call BuiltinAdd                   ; Sparkplug calls interpreter builtin
ret
```

The lack of IR means that the compiler has limited optimisation opportunity, beyond very local peephole optimisations. It also means that we have to port the entire implementation separately to each architecture we support, since there’s no intermediate architecture-independent stage. But, it turns out that neither of these is a problem: a fast compiler is a simple compiler, so the code is pretty easy to port; and Sparkplug doesn’t need to do heavy optimisation, since we have a great optimising compiler later on in the pipeline anyway.

#### Interpreter-compatible frames

Adding a new compiler to an existing mature JavaScript VM is a daunting task. There’s all sorts of things you have to support beyond just standard execution; V8 has a debugger, a stack-walking CPU profiler, there’s stack traces for exceptions, integration into the tier-up, on-stack replacement to optimized code for hot loops… it’s a lot.

Sparkplug does a neat sleight-of-hand that simplifies most of these problems away, which is that it maintains `interpreter-compatible stack frames`.

Sparkplug’s 1:1 mapping doesn’t just relate to how it compiles Ignition’s bytecode down to its machine code variant; it’s also related to stack frames as well. As we know, each portion of the compiler pipeline needs to store function state. And as we’ve seen already in V8, JavaScript function states are stored in Ignition’s stack frames by storing the current function being called, the context it is being called with, the number of arguments that were passed, a pointer to the bytecode array, and so on and so forth.

Ignition is a **register-based interpreter**, where bytecode operands read from and write to a set of virtual registers stored in the stack frame. To remain fast and to avoid implementing its own register allocation, Sparkplug simply reuses Ignition’s virtual register slots inside the stack frame. By doing so, Sparkplug closely mirrors the interpreter’s execution model and stack layout, eliminating the need for any explicit mapping or translation between interpreter frames and compiled frames. As a result, Ignition and Sparkplug stack frames are almost entirely identical.

There is only one notable difference between the two. Ignition needs to track the current `bytecode offset` during execution, so its stack frame includes a slot for the bytecode offset. Sparkplug, however, executes linear machine code generated from the bytecode and therefore does not need to maintain a runtime bytecode offset. Instead, Sparkplug repurposes this slot to cache the `feedback vector`, which is critical for inline caches, profiling, and tier-up decisions to TurboFan.

![](ig_sp_frame.png)

Sparkplug intentionally creates and maintains a frame layout which matches the interpreter’s frame; whenever the interpreter would have stored a register value, Sparkplug stores one too. It does this for several reasons:

- It simplifies Sparkplug compilation; Sparkplug can just mirror the interpreter’s behaviour without having to keep some sort of mapping from interpreter registers to Sparkplug state.
- It also speeds up compilation, since the bytecode compiler has done the hard work of register allocation.
- It makes the integration with the rest of the system almost trivial; the debugger, the profiler, exception stack unwinding, stack trace printing, all these operations do stack walks to discover what the current stack of executing functions is, and all these operations continue working with Sparkplug almost unchanged, because as far as they’re concerned, all they have is an interpreter frame.
- It makes on-stack replacement (OSR) trivial. OSR is when the currently executing function is replaced while executing; currently this happens when an interpreted function is inside a hot loop (where it tiers-up to optimized code for that loop), and when the optimized code deoptimises (where it tiers-down and continues the function’s execution in the interpreter). With Sparkplug frames mirroring interpreter frames, any OSR logic that works for the interpreter will work for Sparkplug; even better, we can swap between the interpreter and Sparkplug code with almost zero frame translation overhead.

### Maglev
In December 2023 [Maglev](https://v8.dev/blog/maglev) has been introduced to address the performance gap between Sparkplug and TurboFan. Maglev generates faster code than Sparkplug and compiles significantly faster than TurboFan. It does so by targeting code that doesn’t get "hot" enough for TurboFan’s optimizations but can still benefit from some level of optimization.

Instead of using the single-pass approach taken by Sparkplug, or build a JIT with an intermediate representation (IR). Maglev uses a traditional static single-assignment (SSA) intermediate representation, using a CFG (control flow graph) rather than TurboFan's more flexible but cache unfriendly sea-of-nodes representation.

#### Prepare
First Maglev does a prepass over the bytecode to find branch targets, including loops, and assignments to variables in loop. This pass also collects liveness information, encoding which values in which variables are still needed across which expressions. This information can reduce the amount of state that needs to be tracked by the compiler later.

Maglev does an abstract interpretation of the frame state, creating SSA nodes representing the results of expression evaluation. Variable assignments are emulated by storing those SSA nodes in the respective abstract interpreter register. In the case of branches and switches, all paths are evaluated.

When multiple paths merge, values in abstract interpreter registers are merged by inserting so-called `Phi` nodes: value nodes that know which value to pick depending on which path was taken at runtime.

Loops can merge variable values “back in time”, with the data flowing backwards from the loop end to the loop header, in the case when variables are assigned in the loop body. That’s where the data from the prepass comes in handy: since we already know which variables are assigned inside loops, we can pre-create loop phis before we even start processing the loop body. At the end of the loop we can populate the phi input with the correct SSA node. This allows the SSA graph generation to be a single forward pass, without needing to "fix up" loop variables, while also minimizing the amount of Phi nodes that need to be allocated.

![](graph.jpg)

#### Known Node Information
To be as fast as possible, Maglev does as much as possible at once. Instead of building a generic JavaScript graph and then lowering that during later optimization phases, which is a theoretically clean but computationally expensive approach, Maglev does as much as possible immediately during graph building.

During graph building Maglev will look at runtime feedback metadata collected during unoptimized execution, and generate specialized SSA nodes for the types observed. If Maglev sees `o.x` and knows from the runtime feedback that `o` always has one specific shape, it will generate an SSA node to check at runtime that `o` still has the expected shape, followed by a cheap LoadField node which does a simple access by offset.

Additionally, Maglev will create a side node to record that it already knows the shape of object `o`, making it unnecessary to check the shape again later. If Maglev later encounters an operation on `o` that lacks feedback for some reason, this information learned during compilation can serve as a secondary source of feedback.

Runtime information can come in many forms. Some information must be checked at runtime, such as the shape check described above. Other information can be used without runtime checks by registering dependencies with the runtime. Constant globals fall into this category: since they are not modified between initialization and when Maglev observes them, Maglev can load their values at compile time and embed them directly into machine code. If the runtime ever mutates such a global, it will invalidate and deoptimize the affected code.

Some kinds of information are "unstable" and can only be relied upon as long as the compiler can guarantee they won’t change. For example, right after allocating an object, Maglev knows it is new and can skip expensive write barriers. Once another allocation becomes possible, the garbage collector may move objects, and these checks must be emitted again. Other kinds of information are "stable": if no object has ever transitioned away from a specific shape, Maglev can register a dependency on that event and avoid rechecking the shape, even across calls to unknown functions with unknown side effects (vulnerabilities could be exist ?).

#### Deoptimization
Because Maglev relies on speculative information that is validated at runtime, Maglev-generated code must support deoptimization. To enable this, Maglev attaches an abstract interpreter frame state to any node that may deoptimize. This frame state describes how interpreter registers map to SSA values at that point in the optimized code.

During code generation, this frame state is lowered into metadata that records how to reconstruct the unoptimized interpreter state from the optimized machine state. When deoptimization is triggered, the deoptimizer consumes this metadata, reads values from machine registers and stack slots, and restores them into the appropriate locations in the interpreter frame. Execution can then safely resume in Ignition.

Maglev builds on the same deoptimization infrastructure used by TurboFan. This allows Maglev to reuse most of the existing logic and benefit from a well-tested deoptimization system, rather than introducing a new mechanism from scratch.

### Turbofan
Finally, we’ll move on to TurboFan, V8’s ultimate and most powerful optimizing JIT compiler. TurboFan bring us interesting intermediate representation concept known as the “Sea of Nodes” with a multi-layered translation and optimization pipeline that helps TurboFan generate better quality machine code from bytecode. In fact, TurboFan is much more than just a compiler.

TurboFan is actually responsible for the interpreter’s bytecode handlers, builtins, code stubs, and inline cache system via it’s [macro assembler](https://source.chromium.org/chromium/chromium/src/+/main:v8/src/codegen/macro-assembler.h)! So we can say that TurboFan is the most important part of the compiler pipeline.

So, how Turbofan work?

Well optimizing compilers work via something called a “[profiler](https://source.chromium.org/chromium/chromium/src/+/main:base/profiler/)”. In essence, this profiler works ahead of time by watching for code that should be optimized (we refer to this code or JavaScript function as being “hot”). It does this by collecting metadata and “samples” from JavaScript functions and the stack by looking at the information collected by [inline caches](https://mathiasbynens.be/notes/shapes-ics) and the feedback vector.

The compiler then builds an intermediate representation (IR) data structure which is used to produce optimized code. This whole process of watching the code and then compiling machine code is called Just-in-Time or JIT compilation.

#### Just-In-Time Compilation (JIT)
As we know, the execution of bytecode in the interpreter VM is slower than assembly execution on the native machine. Reason for this is because JavaScript is dynamic and there is a lot of overhead for property lookups, checking of objects, values, etc. and we’re also running on an emulated stack.

Of course, Maps and Incline Caching (IC) help solve some of these overheads by speeding up dynamic lookup of properties, objects, and values - but they still can’t deliver peak performance. The reason for that is because each IC acts on its own, and it has no knowledge or concept about its neighbors.

Take Maps for example, if we add a property to a known shape, we still have to follow the transitions table and look up or add additional shapes. If we have to do this many times over and over again for a specific function or object, even with a known shape, we’re pretty much wasting computational cycles by doing this time and time again.

So, when there is a JavaScript function that is executed a lot of times, it might be worth spending the time to pass the function into the compiler and compile it down to machine code, allowing it to be executed much faster.

For example, let’s take this code:

```js
function hot_function(obj) {
	return obj.x;
}

for (let i=0; i < 100000000; i++) {
	hot_function({x:i});
}
```

The `hot_function` simply takes an object and returns the value of its x property. We then execute this function around 100,000,000 times (this number depends on the V8 version and may be higher or lower, I use this number to make sure it triggers optimization), each time passing an object whose x property is an integer. Since the function is invoked frequently and the overall shape of the object remains stable, V8 may decide to tier it up the pipeline for compilation, allowing it to execute more efficiently.

We can see this in actions within d8 by tracing the optimization with the `--trace-opt` flag. So, let’s do just that, and also, tack on the `--allow-natives-syntax` command so we can explore how the functions code looks like before and after optimization.

```
C:\BE\CVE-2018-17463\v8\v8\out\x64.debug>d8 --allow-natives-syntax --trace-opt
V8 version 7.1.0 (candidate)
d8> function hot_function(obj) {return obj.x;}
undefined
d8> %DisassembleFunction(hot_function)
000003715E196261: [Code]
 - map: 0x00fa45b02ab1 <Map>
kind = BUILTIN
name = InterpreterEntryTrampoline
compiler = unknown
address = 000003715E196261

Instructions (size = 993)
```

As you can see, initially this code object will be executed by Ignition since it’s a `BUILTIN` and will be handled by the `InterpreterEntryTrampoline` as we know. Now, if we execute this function 100k times, we will see it be optimized by TurboFan.
```
d8> for (let i=0; i < 100000000; i++) {hot_function({x:i});}
[marking 0x0228d44a5891 <JSFunction (sfi = 00000228D44A5701)> for optimized recompilation, reason: small function, ICs with typeinfo: 5/5 (100%), generic ICs: 0/5 (0%)]
[compiling method 0x0228d44a5891 <JSFunction (sfi = 00000228D44A5701)> using TurboFan OSR]
[optimizing 0x0228d44a5891 <JSFunction (sfi = 00000228D44A5701)> - took 5.783, 6.630, 0.123 ms]
99999999
```

Note that the details of the printed information depend on the V8 version.

As you can see that, we are marking the JSFunction’s `SFI` or `SharedFunctionInfo` for optimization. If you remember back to our Ignition, you’ll remember that the `SFI` contains the bytecode for our function. TurboFan will use that bytecode to generate IR and then optimize it down to machine code.

TurboFan does the same thing Sparkplug does when it optimizes bytecode. It will replace the stack frame with a real JIT or system stack frame that will point to the optimized code during runtime. This allows the function to go directly to the optimized code the next time it is called, versus being executed within Ignitions emulated stack.

If we run `%DisassembleFunction` against our hot_function again, we should see that it is now optimized and the code entry point in the SharedFunctionInfo will point to optimized machine code.

As we know, TurboFan didn’t kick in right away, but after a few seconds or after a few thousand iterations of the loop. TurboFan waits for the code to “warm up”. If you remember we briefly mentioned the `feedback vector`. This vector stores the object runtime data along with information from the inline caches and collects what is known as **type feedback**.

This is critical for TurboFan because as we know, JavaScript is dynamic and there is no way for us to store static type information. Second of all, we don’t know the type of a value till runtime. The JIT compiler actually has to make educated guesses about the usage and behavior of the code it’s compiling, such as what your function type is, the type of variables that are being passed in, etc. In essence the compiler makes a lot of assumptions or “speculations”.

This is why optimizing compilers rely on information collected by inline caches and use the feedback vector to make informed decisions about how to optimize the code for performance. This approach is known as Speculative Optimization.
#### Speculative Optimization
So how does speculative optimization help us turn our JavaScript code into highly optimized machine code?

Let’s say we have a simple evaluation for a function called add, such as return 1 + `i`. Here we are returning a value by adding 1 to `i`. Without knowing what type `i` is, we need to follow the ECMAScript standard implementation for the runtime semantic of [EvaluateStringOrNumericBinaryExpression](https://tc39.es/ecma262/#sec-evaluatestringornumericbinaryexpression).

![](ECAMScript-Addition-1.png)

As you can see, once we evaluate the left and right references, and call GetValue on both the left and right values of our operand, we then need to follow the ECMAScript standard to [ApplyStringOrNumericBinaryOperator](https://tc39.es/ecma262/#sec-applystringornumericbinaryoperator) so we can return our value.

![](ECAMScript-Addition-2.png)

Without knowing the type of variable `i` is, be it an integer or string, there is no way we can implement this whole evaluation in just a few machine instructions, and nonetheless have it be fast. This is where the speculative optimization comes in, where TurboFan will rely on the feedback vector to make its assumptions about the possible types that `i` is.

For example, if after a few hundred runs we look at the feedback vector for the Add bytecode, and know that `i` is a number, then we don’t have to handle the [ToString](https://tc39.es/ecma262/#sec-tostring) or even the [ToPrimitive](https://tc39.es/ecma262/#sec-toprimitive) evaluations. In that case, the optimizer can take an IR instruction and claim that i and the return value are just numbers and load it as such. Which minimizes the amount of machine instructions we need to generate.

If you remember back to the mention of the `JSFunction` object or closure, you’ll remember that the closure linked us to the `feedback vector` slot as well as the `SharedFunctionInfo`. Within the feedback vector, there is an interesting slot called the `BinaryOp` slot, which records feedback about the inputs and outputs of binary operations such as `+`, `-`, `*`, etc.

We can check what’s inside our feedback vector and see this specific slot by running %DebugPrint against our add function.
```
C:\BE\CVE-2018-17463\v8\v8\out\x64.debug>d8 --allow-natives-syntax
V8 version 7.1.0 (candidate)
d8> function add(i) {return 1 + i;}
undefined
d8> for (let i=0; i<100; i++) {add(i);}
100
d8> %DebugPrint(add)
DebugPrint: 00000341D0B22279: [Function] in OldSpace
 - map: 0x0338afd02481 <Map(HOLEY_ELEMENTS)> [FastProperties]
 - prototype: 0x0341d0b042a1 <JSFunction (sfi = 0000013335B84DE9)>
 - elements: 0x01a577f02cf1 <FixedArray[0]> [HOLEY_ELEMENTS]
 - function prototype:
 - initial_map:
 - shared_info: 0x0341d0b220e1 <SharedFunctionInfo add>
 - name: 0x01a577f04b91 <String[3]: add>
 - formal_parameter_count: 1
 - kind: NormalFunction
 - context: 0x0341d0b039f9 <NativeContext[248]>
 - code: 0x031478c96261 <Code BUILTIN InterpreterEntryTrampoline>
 - interpreted
 - bytecode: 00000341D0B255C9
 - source code: (i) {return 1 + i;}
 - properties: 0x01a577f02cf1 <FixedArray[0]> {
    #length: 0x013335b9a3b9 <AccessorInfo> (const accessor descriptor)
    #name: 0x013335b9a349 <AccessorInfo> (const accessor descriptor)
    #arguments: 0x013335b9a269 <AccessorInfo> (const accessor descriptor)
    #caller: 0x013335b9a2d9 <AccessorInfo> (const accessor descriptor)
    #prototype: 0x013335b9a429 <AccessorInfo> (const accessor descriptor)
 }

 - feedback vector: 00000341D0B25621: [FeedbackVector] in OldSpace
 - map: 0x01a577f02c91 <Map>
 - length: 1
 - shared function info: 0x0341d0b220e1 <SharedFunctionInfo add>
 - optimized code/marker: OptimizationMarker::kNone
 - invocation count: 100
 - profiler ticks: 0
 - slot #0 BinaryOp BinaryOp:SignedSmall {
     [0]: 1
  }
```
There are a few interesting items in here. `Invocation` count shows us the number of times we ran the `add` function, and if we look into our feedback vector, you’ll see that we have exactly one slot, which is the `BinaryOp` that we talked about. Looking into that slot we see that it contains the current feedback type of `SignedSmall` which in essence refers to an `SMI`.

Overall, these speculations via feedback vectors are great in helping speed up our code by removing unnecessary machine instructions for different types. Unfortunately, it’s pretty unsafe to just apply instructions solely focused around one type to dynamic objects.

So, what happens if halfway during the optimized function we pass in a string instead of a number? In essence, if this was to happen then we would have a type confusion vulnerability on our hands. To protect against potentially wrong assumptions, TurboFan prepends something known as a `type guard` before execution of specific instructions.

This type guard checks to make sure that the shape of the object we are passing in is the correct type. This is done before the object reaches our optimized operations. If the object does not match the expected shape, then the execution of the optimized code can’t continue. In that case, we will “bail out” of the assembly code, and jump back to the unoptimized bytecode within the interpreter and continue execution there. This is known as “deoptimization”. Deoptimizations due to type guards aren’t just limited in checking if there is a mismatch in object types. They also work on arithmetic operations and bound checking.

Let's continue with the following example.

```js
for (let i=0; i<100000000; i++) {
	if (i<700000) {
		add(i);
	} else {
		add("string");
	}
}
```
As with the optimization process, we can also see deoptimization in action within `d8` by utilizing the `--trace-deopt` flag.

```
C:\BE\CVE-2018-17463\v8\v8\out\x64.debug>d8 --allow-natives-syntax --trace-opt --trace-deopt
V8 version 7.1.0 (candidate)
d8> function add(i) {return 1 + i;}
undefined
d8> for (let i=0; i<100000000; i++) {if (i<700000) {add(i);} else {add("string");}}
[marking 0x001be5e25d31 <JSFunction (sfi = 0000001BE5E25BB9)> for optimized recompilation, reason: small function, ICs with typeinfo: 5/6 (83%), generic ICs: 0/6 (0%)]
[compiling method 0x001be5e25d31 <JSFunction (sfi = 0000001BE5E25BB9)> using TurboFan OSR]
[optimizing 0x001be5e25d31 <JSFunction (sfi = 0000001BE5E25BB9)> - took 1.941, 4.750, 0.124 ms]
[deoptimizing (DEOPT soft): begin 0x001be5e25d31 <JSFunction (sfi = 0000001BE5E25BB9)> (opt #1) @3, FP to SP delta: 80, caller sp: 0x0088eb3fe828]
  ...
[deoptimizing (soft): end 0x001be5e25d31 <JSFunction (sfi = 0000001BE5E25BB9)> @3 => node=52, pc=0x7ff7097f38e0, caller sp=0x0088eb3fe828, took 4.680 ms]
[compiling method 0x001be5e25d31 <JSFunction (sfi = 0000001BE5E25BB9)> using TurboFan OSR]
[optimizing 0x001be5e25d31 <JSFunction (sfi = 0000001BE5E25BB9)> - took 2.269, 7.691, 0.148 ms]
"1string"
```

As you can see, the functions gets optimized, and later we trigger a bailout. This deoptimizes the code back to bytecode due to an insufficient type during our call. Then, something interesting happens. The function gets optimized again. Why?

Well, the function is still “hot” and there are a few more thousand iterations to go. What TurboFan will do, now that it collected both a number and string within the type feedback, is that it will go back and optimize the code for a second time. But this time it will add code which will allow for string evaluation. In this case, a second type guard will be added - so the second run of code is now optimized for both a number and a string!

We can also see this updated feedback in the BinaryOp slot by running the `%DebugPrint` command against our add function. You should see something similar as below.`

```
d8> %DebugPrint(add)
DebugPrint: 0000001BE5E22279: [Function] in OldSpace
 - map: 0x010f6ef82481 <Map(HOLEY_ELEMENTS)> [FastProperties]
 - prototype: 0x001be5e042a1 <JSFunction (sfi = 000000D62D804DE9)>
 - elements: 0x017b51702cf1 <FixedArray[0]> [HOLEY_ELEMENTS]
 - function prototype:
 - initial_map:
 - shared_info: 0x001be5e220e1 <SharedFunctionInfo add>
 - name: 0x017b51704b91 <String[3]: add>
 - formal_parameter_count: 1
 - kind: NormalFunction
 - context: 0x001be5e039f9 <NativeContext[248]>
 - code: 0x02de36116261 <Code BUILTIN InterpreterEntryTrampoline>
 - interpreted
 - bytecode: 0000001BE5E25619
 - source code: (i) {return 1 + i;}
 - properties: 0x017b51702cf1 <FixedArray[0]> {
    #length: 0x00d62d81a3b9 <AccessorInfo> (const accessor descriptor)
    #name: 0x00d62d81a349 <AccessorInfo> (const accessor descriptor)
    #arguments: 0x00d62d81a269 <AccessorInfo> (const accessor descriptor)
    #caller: 0x00d62d81a2d9 <AccessorInfo> (const accessor descriptor)
    #prototype: 0x00d62d81a429 <AccessorInfo> (const accessor descriptor)
 }

 - feedback vector: 0000001BE5E25671: [FeedbackVector] in OldSpace
 - map: 0x017b51702c91 <Map>
 - length: 1
 - shared function info: 0x001be5e220e1 <SharedFunctionInfo add>
 - optimized code/marker: OptimizationMarker::kNone
 - invocation count: 19839
 - profiler ticks: 1
 - slot #0 BinaryOp BinaryOp:Any {
     [0]: 127
  }
0000010F6EF82481: [Map]
```
As you can see, the BinaryOp now stores the feedback type of Any, instead of SignedSmall and String. Why? Well, this is due to something called the **Feedback Lattice**.

#### Feedback Lattice
The feedback lattice represents the possible feedback states for a given operation. It starts in the None state, indicating that no information has been observed yet, and gradually moves toward the Any state as more diverse inputs and outputs are seen. Reaching the Any state means the operation is considered polymorphic, while any other state indicates a monomorphic behavior, since only a single, consistent value has been observed.

![](lattice-20171213.jpg)

Feedback can only progress downwards in the lattice. Once we go from Number to Any, we can never go back. If we do go back for some magic reason, then we risk entering a so-called deoptimization loop where the optimizing compiler consumes invalid feedback and bails out from optimized code continuously.

#### "Sea of Nodes” Intermediate Representation (IR)
Now that we know how type feedback is collected for TurboFan to make its speculative assumptions, let’s see how TurboFan builds its specialized IR from this feedback. The reason that IR is generated is due to the fact that this data structure abstracts from code complexity, which in turn makes it easier to preform compiler optimizations.

Now, TurboFans “Sea of Nodes” IR is based on static single assignment or SSA, which is a property of IR that requires each variable to be assigned exactly once and defined before it is used. This is useful for optimizations such as [redundancy elimination](https://en.wikipedia.org/wiki/Partial-redundancy_elimination).

```js
function hot_function(obj) {
	return obj.x;
}

for (let i=0; i < 100000000; i++) {
	hot_function({x:i});
}
```

Once done, we will run this script via `d8` with the `--trace-turbo` flag which allows us to trace and save the IR generated by TurboFans JIT. At the end of the run, it should generate a JSON file that has the naming convention of `turbo-*.json`.

```
C:\BE\CVE-2018-17463\v8\v8\out\x64.debug>d8 --trace-turbo hot.js
Concurrent recompilation has been disabled for tracing.
---------------------------------------------------
Begin compiling method  using Turbofan
---------------------------------------------------
Finished compiling method  using Turbofan
```

After that’s completed, navigate to [Turbolizer](https://v8.github.io/tools/head/turbolizer/index.html) in a web browser. This tool will help us visualize the sea of nodes graph generated by TurboFan.

![](turbo1.png)

In Turbolizer, on the left you’ll see your source code, and the right side, you’ll have the optimized machine code that was generated by TurboFan. In the middle you will have the sea of nodes graph.

Now, if you look at the top left portion of the sea of nodes window, you’ll see that we are in the `Bytecode Graph Builder` option. This option shows us the generated IR from bytecode without any optimizations applied to it. From the drop-down menu we can select the other different optimization passes that this code goes through to view the associated IR.
![](turbo2.png)

#### Common Optimizations

Ok, now that we covered TurboFans Sea of Nodes, we should have at least a decent understanding of how to navigate and understand the generated IR. From here we can dive into understanding some of TurboFans common optimizations. These optimizations in essence act on the original graph that was produce from bytecode.

Since the resulting graph now has static type information due to type guards, the optimizations are done in a more classic ahead-of-time fashion to improve the execution speed or memory footprint of the code. Afterwards, once the graph is optimized, the resulting graph is lowered to machine code (known as “lowering”) and is then written into an executable memory region for V8 to execute when the compiled function is called.

One thing to note, is that lowering can happen in multiple stages with further optimizations in between, making this compiler pipeline pretty flexible. With that being said, let’s take a look at some common optimizations.

I won’t go into the details here, as they can be explored further elsewhere or in other dedicate posts. However, it’s important to keep in mind that if the side effects of these optimizations are not handled correctly, critical checks may be eliminated — often leading to vulnerabilities such as **Type Confusion** or **Out-of-bound**.

- **Typer**: One of the earliest optimization phases is called the `TyperPhase` which is ran by the `OptimizeGraph` function. This phase traces through the code and identifies the resulting types of operations from heap objects, such as Int32 + Int32 = Int32.

- **Range Analysis**: `Range analysis` allows the compiler to infer the possible range of values ahead of time, enabling it to generate faster machine code. When these assumptions turn out to be incorrect at runtime, deoptimization is used as a safety mechanism to fall back to a correct execution path.

- **Bounds Checking Elimination (BCE)**: Another common optimization that was applied with the `Typer` during the simplified lowering phase was the `CheckBounds` operation which is applied to `CheckBound` speculative nodes. This optimization is usually applied to array access operations if the index of the array has been proven to be within the bounds of the array after range analysis. In  fact, the Chromium team has decided to disable this optimization in order to harden TurboFan’s bounds check against typer bugs. You can read [Circumventing Chrome's hardening of typer bugs](https://doar-e.github.io/blog/2019/05/09/circumventing-chromes-hardening-of-typer-bugs/).

- **Redundancy Elimination**: Redundancy Elimination is an optimization that removes redundant checks (primarily type checks, bounds checks, map checks, ...) when they are already guaranteed by earlier checks in the effect chain. It is conceptually similar to Bounds Check Elimination (BCE), but operates on a broader scope, covering multiple kinds of runtime checks rather than just array bounds. I’ll discuss this in more detail in a later post analyzing CVE-2018-17463, where Redundancy Elimination plays a critical role in triggering the vulnerability.
- **Control Optimization**: This optimization works on optimizing the flow of the graph and turns certain branch chains into switches.
- **Dead Code Elimination (DCE)**: Dead Code Elimination is exactly what it sounds like. It simply iterates through all the nodes and removes the code that won’t be executed.

## Reference
[TurboFan: A new code generation architecture for V8](https://docs.google.com/presentation/d/1_eLlVzcj94_G4r9j9d_Lj5HRKFnq6jgpuPJtnmIBs88/edit?slide=id.g2134da681e_0_59#slide=id.g2134da681e_0_59)

[Ignition Design Doc](https://docs.google.com/document/d/11T2CRex9hXxoJwbYqVQ32yIPMh0uouUZLdyrtmMoL44/edit?tab=t.0#heading=h.6jz9dj3bnr8t)

[JavaScript engine fundamentals: optimizing prototypes](https://mathiasbynens.be/notes/prototypes)

[Design of V8 bindings](https://chromium.googlesource.com/chromium/src/+/master/third_party/blink/renderer/bindings/core/v8/V8BindingDesign.md)

[Maglev - V8’s Fastest Optimizing JIT ](https://v8.dev/blog/maglev)

[An Introduction to Chrome Exploitation - Maglev Edition](https://www.matteomalvica.com/blog/2024/06/05/intro-v8-exploitation-maglev/)

[An Introduction to Speculative Optimization in V8](https://benediktmeurer.de/2017/12/13/an-introduction-to-speculative-optimization-in-v8/)

[TurboFan JIT Design](https://docs.google.com/presentation/d/1sOEF4MlF7LeO7uq-uThJSulJlTh--wgLeaVibsbb3tc/htmlpresent)

[Introduction to TurboFan](https://doar-e.github.io/blog/2019/01/28/introduction-to-turbofan/)

[Chrome Browser Exploitation, Part 2: Introduction to Ignition, Sparkplug and JIT Compilation via TurboFan](https://jhalon.github.io/chrome-browser-exploitation-2/)